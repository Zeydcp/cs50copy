# Experimentation Process

I began by trying one convolutional layer with 32 filters, using a 3x3 kernel and ReLU activation. One Max-pooling layer, using 2x2 pool size and One hidden layer with 128 units, ReLU activation and a dropout of 0.5, and then an output layer with 43 units and Sigmoid activation. This had accuracy of less than 1%. I then decided to add another dense layer with 256 units, ReLU activation and a dropout of 0.5. This did not improve the accuracy so I removed it. I then decided to add another convolutional layer with 64 filters, using a 3x3 kernel, ReLU activation and another max pooling layer using 2x2 pool size. This improved accuracy to over 95% on the testing set. Accuracy was improving steadily after each epoch and flattened out in the last two epochs. I was adament on using 0.5 dropout to ensure that there was no over-relyance on any neurons.
